{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e259ac3-7578-4e66-9a50-c396c1e47f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from urllib.parse import unquote, quote #для извлечения slug'ов из URL\n",
    "from tqdm import tqdm #Для прогресс-бара\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71683052-c071-42b9-aaad-6931f660c034",
   "metadata": {},
   "source": [
    "Для разбиения на чанки была использована предложенная бибилиотека: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/ \n",
    "Для разделения основного текста с максимальным сохранением смысла из каждого абзаца использовали RecursiveCharacterTextSplitter: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e71b0ef3-c6a3-41f8-a6d8-0c49bb94d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAGE_LIMIT = 50 # Количество статей для обработки (мы парсили 68000)\n",
    "OUTPUT_FILE = \"starwars_dataset.json\"  # Имя выходного файла\n",
    "BASE_URL = \"https://starwars.fandom.com/api.php\"\n",
    "HEADERS = {'User-Agent': 'FandomDataCollector/1.0'}\n",
    "\n",
    "# Разбиение на чанки\n",
    "CHUNK_SIZE = 1850 # Размер чанка в символах, в одном английском слове в среднем 5 символов\n",
    "CHUNK_OVERLAP = 320  # Перекрытие чанков, примерно 10-15 процентов от общего объема чанка\n",
    "TEXT_SPLITTER = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea9159f-b574-4507-93e7-26ae8e36e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_slug(slug):\n",
    "    #Нормализация названий для использования в качестве ID\n",
    "    slug = unquote(slug)  # Декодируем URL-спецсимволы\n",
    "    slug = re.sub(r\"#.*$\", \"\", slug)  # Удаляем якоря\n",
    "    slug = re.sub(r\"[^\\w\\-]\", \"\", slug)  # Удаляем спецсимволы\n",
    "    return slug.replace(\" \", \"_\").lower()  # Единый формат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9b89f6-4b05-4487-9e87-5b72551b03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages(limit=PAGE_LIMIT):\n",
    "    #Получение списка страниц с возможностью ограничения количества\n",
    "    all_pages = []\n",
    "    apcontinue = None\n",
    "    progress = tqdm(desc=\"Сбор списка статей\", unit=\"page\")\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"allpages\",\n",
    "            \"apnamespace\": 0,\n",
    "            \"aplimit\": \"max\",\n",
    "            \"apfilterredir\": \"nonredirects\"  # Только не-перенаправления\n",
    "        }\n",
    "        if apcontinue:\n",
    "            params[\"apcontinue\"] = apcontinue\n",
    "\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "        data = response.json()\n",
    "        batch = data['query']['allpages']\n",
    "        \n",
    "        # Фильтрация странных статей\n",
    "        valid_batch = [\n",
    "            p for p in batch \n",
    "            if not p['title'].startswith(('\"', \"'\", \":\", \"!\")) \n",
    "            and len(p['title']) > 2\n",
    "        ]\n",
    "        \n",
    "        # Применяем лимит\n",
    "        if limit and len(all_pages) + len(valid_batch) > limit:\n",
    "            all_pages.extend(valid_batch[:limit - len(all_pages)])\n",
    "            progress.update(len(valid_batch[:limit - len(all_pages)]))\n",
    "            break\n",
    "        else:\n",
    "            all_pages.extend(valid_batch)\n",
    "            progress.update(len(valid_batch))\n",
    "\n",
    "        if 'continue' in data and (not limit or len(all_pages) < limit):\n",
    "            apcontinue = data['continue']['apcontinue']\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    progress.close()\n",
    "    return all_pages[:limit] if limit else all_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d7d081-39cd-437e-a92f-96cc48e01a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(title):\n",
    "    #Полный парсинг страницы с несколькими уровнями отказоустойчивости\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Запрос к API\n",
    "        response = requests.get(BASE_URL, headers=HEADERS, params=params, timeout=15)\n",
    "        \n",
    "        # Проверка статуса ответа\n",
    "        if response.status_code != 200:\n",
    "            print(f\"API вернул статус {response.status_code} для '{title}'\")\n",
    "            # Попробуем прямой запрос к HTML-странице (иной способ парсинга)\n",
    "            return parse_page_direct(title)\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Извлечение канонического URL\n",
    "        canonical_url = data.get(\"parse\", {}).get(\"canonicalurl\", \"\")\n",
    "        if not canonical_url:\n",
    "            canonical_url = f\"https://starwars.fandom.com/wiki/{quote(title.replace(' ', '_'), safe='')}\"\n",
    "        \n",
    "        # Попытка получить HTML через API\n",
    "        html = data.get(\"parse\", {}).get(\"text\", {}).get(\"*\", None)\n",
    "        \n",
    "        # Если HTML не получен через API - переходим к прямому парсингу\n",
    "        if not html:\n",
    "            print(f\" Не удалось получить HTML через API для '{title}'\")\n",
    "            return parse_page_direct(title, canonical_url)\n",
    "        \n",
    "        # Первичный парсинг BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "        \n",
    "        # Если основной контент не найден - попробуем альтернативные методы\n",
    "        if not content:\n",
    "            print(f\" Основной контент не найден для '{title}' через API\")\n",
    "            return parse_page_direct(title, canonical_url)\n",
    "        \n",
    "        # Очистка контента с улучшенной обработкой ошибок\n",
    "        elements_to_remove = []\n",
    "        for element in content.find_all([\"table\", \"div\", \"span\"]):\n",
    "            # Улучшенная проверка элементов\n",
    "            if not element or not hasattr(element, 'get'):\n",
    "                continue\n",
    "                \n",
    "            classes = element.get(\"class\", [])\n",
    "            if not classes: \n",
    "                continue\n",
    "                \n",
    "            # Проверяем классы через множества для безопасности\n",
    "            class_set = set(classes)\n",
    "            if class_set & {\"infobox\", \"navbox\", \"metadata\", \"toc\", \"portable-infobox\"}:\n",
    "                elements_to_remove.append(element)\n",
    "        \n",
    "        # Удаляем после цикла, чтобы не нарушать итерацию\n",
    "        for element in elements_to_remove:\n",
    "            element.decompose()\n",
    "        \n",
    "        # Извлечение текста и ссылок с резервными методами\n",
    "        full_text = \"\"\n",
    "        link_map = {}\n",
    "        \n",
    "        # Сначала попробуем стандартные элементы\n",
    "        elements = content.find_all([\"p\", \"h2\", \"h3\", \"h4\", \"ul\", \"ol\"])\n",
    "        \n",
    "        # Если не найдено - попробуем более агрессивный подход\n",
    "        if not elements:\n",
    "            print(f\"Стандартные элементы не найдены для '{title}', пробуем все содержимое\")\n",
    "            elements = [content]\n",
    "        \n",
    "        for element in elements:\n",
    "            try:\n",
    "                elem_text = \"\"\n",
    "                # Используем get_text() как резервный метод\n",
    "                elem_text = element.get_text(separator=\" \", strip=True)\n",
    "                \n",
    "                # Дополнительно извлекаем ссылки\n",
    "                for link in element.find_all(\"a\", href=True):\n",
    "                    href = link.get(\"href\", \"\")\n",
    "                    if \"/wiki/\" in href:\n",
    "                        slug = href.split(\"/wiki/\")[-1].split(\"#\")[0]\n",
    "                        normalized_slug = normalize_slug(slug)\n",
    "                        anchor_text = link.get_text(strip=True)\n",
    "                        if anchor_text:\n",
    "                            link_map[anchor_text] = normalized_slug\n",
    "                \n",
    "                full_text += elem_text + \"\\n\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\" Ошибка при обработке элемента: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Проверка минимального контента\n",
    "        if len(full_text.strip()) < 50:\n",
    "            print(f\"Мало контента ({len(full_text)} симв.) для '{title}', пробуем прямой метод\")\n",
    "            return parse_page_direct(title, canonical_url)\n",
    "        \n",
    "        return full_text.strip(), link_map, canonical_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Критическая ошибка при парсинге {title}: {str(e)}\")\n",
    "        # Последняя попытка - прямой парсинг\n",
    "        return parse_page_direct(title)\n",
    "\n",
    "def parse_page_direct(title, canonical_url=None):\n",
    "    #Альтернативный метод парсинга через прямой запрос к странице\n",
    "    try:\n",
    "        if not canonical_url:\n",
    "            canonical_url = f\"https://starwars.fandom.com/wiki/{quote(title.replace(' ', '_'), safe='')}\"\n",
    "        \n",
    "        print(f\"Пробуем прямой парсинг для '{title}'\")\n",
    "        response = requests.get(canonical_url, headers=HEADERS, timeout=15)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Основной контент - попробуем разные варианты\n",
    "        content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "        if not content:\n",
    "            # Резервный поиск\n",
    "            content = soup.find(\"div\", {\"id\": \"content\"})\n",
    "        \n",
    "        if not content:\n",
    "            print(f\"Прямой парсинг не удался для '{title}'\")\n",
    "            return None, None, canonical_url\n",
    "        \n",
    "        # Упрощенная очистка\n",
    "        for element in content.find_all(class_=[\"portable-infobox\", \"infobox\", \"navbox\", \"toc\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Извлечение текста\n",
    "        full_text = content.get_text(separator=\"\\n\", strip=True)\n",
    "        \n",
    "        # Извлечение ссылок\n",
    "        link_map = {}\n",
    "        for link in content.find_all(\"a\", href=True):\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if \"/wiki/\" in href:\n",
    "                slug = href.split(\"/wiki/\")[-1].split(\"#\")[0]\n",
    "                normalized_slug = normalize_slug(slug)\n",
    "                anchor_text = link.get_text(strip=True)\n",
    "                if anchor_text:\n",
    "                    link_map[anchor_text] = normalized_slug\n",
    "        \n",
    "        return full_text.strip(), link_map, canonical_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при прямом парсинге '{title}': {str(e)}\")\n",
    "        return None, None, canonical_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30bf8df-d402-4a67-b4d7-653c6c052d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunk_links(chunk_text, link_map):\n",
    "    #Извлечение ссылок, релевантных конкретному чанку\n",
    "    found_links = set()\n",
    "    for anchor_text, slug in link_map.items():\n",
    "        # Поиск с учетом границ слов для точного соответствия\n",
    "        if re.search(rf\"\\b{re.escape(anchor_text)}\\b\", chunk_text, re.IGNORECASE):\n",
    "            found_links.add(slug)\n",
    "    return list(found_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86f9c939-b782-4798-9805-fe3ca464e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Сбор и обработка данных\n",
    "    print(f\"Конфигурация: Лимит статей = {PAGE_LIMIT or 'Без ограничений'}\")\n",
    "    print(f\"             Выходной файл = {OUTPUT_FILE}\")\n",
    "    print(f\"             Размер чанка = {CHUNK_SIZE} симв.\")\n",
    "    \n",
    "    pages = get_all_pages()\n",
    "    dataset = []\n",
    "    \n",
    "    print(f\"\\nНачата обработка {len(pages)} статей...\")\n",
    "    progress_bar = tqdm(total=len(pages), desc=\"Обработка статей\", unit=\"статья\")\n",
    "    \n",
    "    for page in pages:\n",
    "        title = page['title']\n",
    "        doc_slug = normalize_slug(title)\n",
    "        \n",
    "        try:\n",
    "            full_text, link_map, canonical_url = parse_page(title)\n",
    "            if not full_text or not link_map or not canonical_url:\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "                \n",
    "            # Разбиение на чанки\n",
    "            chunks = TEXT_SPLITTER.split_text(full_text)\n",
    "            \n",
    "            # Формирование записей для каждого чанка\n",
    "            for chunk_id, chunk_text in enumerate(chunks, start=1):\n",
    "                chunk_links = extract_chunk_links(chunk_text, link_map)\n",
    "                \n",
    "                dataset.append({\n",
    "                    \"document_id\": doc_slug,  # Одинаковый для всех чанков статьи\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"title\": title,\n",
    "                    \"chunk_text\": chunk_text,\n",
    "                    \"outgoing_links\": chunk_links,\n",
    "                    \"source\": canonical_url,  # Одинаковый для всех чанков статьи\n",
    "                    \"comment\": f\"Всего чанков: {len(chunks)}\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nКритическая ошибка при обработке {title}: {str(e)}\")\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        time.sleep(0.2)  # Защита от блокировки\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Сохранение в JSON\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Отчет о выполнении\n",
    "    total_articles = len(set(d['document_id'] for d in dataset))\n",
    "    print(f\"\\n Готово! Обработано статей: {total_articles}\")\n",
    "    print(f\" Создано чанков: {len(dataset)}\")\n",
    "    print(f\" Средняя длина чанка: {sum(len(d['chunk_text']) for d in dataset)//max(1, len(dataset))} символ.\")\n",
    "    print(f\" Данные сохранены в: {OUTPUT_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1771a4e1-e72c-4185-987b-0b4a91797626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация: Лимит статей = 50\n",
      "             Выходной файл = starwars_dataset_checking.json\n",
      "             Размер чанка = 1850 симв.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Сбор списка статей: 0page [00:00, ?page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начата обработка 50 статей...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка статей: 100%|██████████| 50/50 [00:37<00:00,  1.33статья/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Готово! Обработано статей: 50\n",
      " Создано чанков: 255\n",
      " Средняя длина чанка: 1319 символ.\n",
      " Данные сохранены в: starwars_dataset_checking.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9fa13-7ea4-4650-96b7-b5bf4c316906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
